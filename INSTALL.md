# Инструкция по установке и запуску Mind-Fix

Это руководство поможет вам развернуть и запустить проект локально.

## Шаг 1: Подготовка

1.  **Установите Docker и Docker Compose**: Убедитесь, что на вашей системе установлен [Docker Desktop](https://www.docker.com/products/docker-desktop/) или отдельные компоненты Docker.
2.  **Установите Python**: Убедитесь, что у вас установлен Python 3.10+ для запуска локальных скриптов.
3.  **Клонируйте репозиторий**: `git clone <URL вашего репозитория>` и перейдите в папку проекта.

## Шаг 2: Конфигурация

1.  **Создайте файл `.env`**: Скопируйте `.env.example` в новый файл `.env`.
    ```bash
    cp .env.example .env
    ```
2.  **Настройте `.env`**: Откройте файл `.env` и впишите ваш токен для Telegram-бота в переменную `TELEGRAM_BOT_TOKEN`. Остальные параметры можно оставить по умолчанию для локального запуска.

## Шаг 3: Наполнение базы знаний

1.  **Поместите файлы в `data/`**: Перейдите в папку `data/` и поместите туда ваши материалы по ИТН: `Полный ИТН.docx`, `Адаптология.pdf`, `result.json` и любые другие `.txt`, `.md` файлы.
2.  **Установите зависимости для скриптов**:
    ```bash
    pip install -r rag_pipeline/requirements.txt
    ```
3.  **Запустите сервисы Ollama и ChromaDB**:
    ```bash
    docker-compose up -d ollama chromadb
    ```
    Дождитесь, пока сервисы полностью запустятся (это может занять несколько минут).
4.  **(Опционально) Скачайте модель Llama 3.1**: Если модель еще не скачана, выполните:
    ```bash
    docker-compose exec ollama ollama pull llama3.1:8b
    ```
5.  **Создайте файл базы знаний**:
    ```bash
    python rag_pipeline/build_knowledge_base.py
    ```
    Этот скрипт обработает все файлы в `data/` и создаст `data/knowledge_base.jsonl`.
6.  **Загрузите базу знаний в ChromaDB**:
    ```bash
    python rag_pipeline/ingest_to_chroma.py
    ```
    Этот скрипт создаст векторный индекс в ChromaDB.

## Шаг 4: Запуск проекта

1.  **Запустите все остальные сервисы**:
    ```bash
    docker-compose up --build -d
    ```
    Эта команда соберет образы для `backend`, `telegram_bot` и `analytics` и запустит все сервисы в фоновом режиме.

## Шаг 5: Проверка работоспособности

1.  **Проверьте API**: Откройте в браузере `http://localhost:8000/docs`. Это Swagger UI для вашего API.
2.  **Отправьте тестовый запрос**:
    *   Найдите эндпоинт `POST /api/v1/rag/query`.
    *   Нажмите "Try it out".
    *   Введите в тело запроса:
        ```json
        {
          "text": "расскажи про весы души",
          "doc_type": "theory"
        }
        ```
    *   Нажмите "Execute". Вы должны получить ответ, сгенерированный LLM на основе ваших документов.
3.  **Проверьте бота**: Напишите вашему Telegram-боту. (Логика его взаимодействия с API еще не реализована, но сервис должен быть запущен).
4.  **Проверьте аналитику**: Откройте `http://localhost:8501`.

## Как обновлять базу знаний

1.  **Добавьте новые файлы**: Просто поместите новые `.docx`, `.pdf` и другие файлы в папку `data/`.
2.  **Повторите шаги 3.5 и 3.6**: Повторно запустите скрипты обработки и индексации, чтобы обновить базу знаний.
    ```bash
    python rag_pipeline/build_knowledge_base.py
    python rag_pipeline/ingest_to_chroma.py
    ```
    Скрипт `ingest_to_chroma.py` перед загрузкой удаляет старую коллекцию, гарантируя чистоту данных. 